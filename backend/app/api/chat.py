# backend/app/api/chat.py (Enhanced for RAG with Page Navigation & Cache Integration)
from fastapi import APIRouter, HTTPException, Query
from app.models.schemas import ChatRequest, ChatResponse
from app.services.document_processor import vector_collection, PDFFileManager, get_document_pdf_path
from app.services.vectorizer import get_embeddings
from app.services.supabase_client import supabase
from app.services.pdf_service import find_text_locations, create_highlighted_pdf, create_rag_response_with_pages
import openai
import time
import os
import tempfile
from typing import List, Dict, Any, Optional
import traceback

router = APIRouter()


@router.post("/chat", response_model=ChatResponse)
async def chat_with_documents(request: ChatRequest):
    """
    ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (í˜ì´ì§€ ìœ„ì¹˜ ì •ë³´ í¬í•¨, ìºì‹œ í™œìš©)
    """
    start_time = time.time()

    try:
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = get_embeddings([request.message])[0]

        # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰
        search_kwargs = {
            "query_embeddings": [query_embedding],
            "n_results": request.max_results
        }

        # íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
        if request.document_ids:
            search_kwargs["where"] = {
                "document_id": {"$in": request.document_ids}
            }

        search_results = vector_collection.query(**search_kwargs)

        if not search_results["documents"] or not search_results["documents"][0]:
            return ChatResponse(
                status="success",
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                sources=[],
                document_ids=[],
                processing_time=time.time() - start_time
            )

        # 3. ê²€ìƒ‰ëœ ì²­í¬ë“¤ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ì†ŒìŠ¤ ì •ë³´ ë³´ê°•
        documents = search_results["documents"][0]
        metadatas = search_results["metadatas"][0]
        distances = search_results["distances"][0]

        context_chunks = []
        sources = []
        document_ids = set()
        
        # ë¬¸ì„œ ì •ë³´ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•˜ê¸° ìœ„í•œ ì¤€ë¹„
        referenced_doc_ids = list(set([meta.get("document_id") for meta in metadatas if meta.get("document_id")]))
        document_info_map = {}
        if referenced_doc_ids:
            try:
                doc_infos_response = supabase.from_('documents').select('id, url, filename').in_('id', referenced_doc_ids).execute()
                for doc_info in doc_infos_response.data:
                    document_info_map[doc_info['id']] = doc_info
            except Exception as db_error:
                print(f"âŒ ì°¸ì¡°ëœ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {db_error}")


        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
            if distance < 0.8:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                context_chunks.append(doc)
                document_id = metadata.get("document_id", "")
                document_ids.add(document_id)

                # ë©”íƒ€ë°ì´í„°ì—ì„œ í˜ì´ì§€ ì •ë³´ ì§ì ‘ ì¶”ì¶œ (ìºì‹œ í™œìš©)
                start_page = metadata.get("start_page", 1)
                end_page = metadata.get("end_page", 1)
                chunk_index = metadata.get("chunk_index", i)

                # ì¡°íšŒëœ ë¬¸ì„œ ì •ë³´ë§µì—ì„œ URLê³¼ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
                doc_info = document_info_map.get(document_id, {})
                
                source_info = {
                    "document_id": document_id,
                    "filename": doc_info.get("filename", "Unknown"),
                    "url": doc_info.get("url", ""),  # URL ì¶”ê°€
                    "chunk_index": chunk_index,
                    "relevance_score": 1 - distance,
                    "content_preview": doc[:200] + "..." if len(doc) > 200 else doc,
                    "full_content": doc,
                    "page_number": start_page,
                    "end_page": end_page,
                    "bbox": None,  # í•„ìš”ì‹œ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚°
                    "chunk_hash": metadata.get("chunk_hash", ""),
                    "char_count": metadata.get("char_count", len(doc))
                }

                sources.append(source_info)

        if not context_chunks:
            return ChatResponse(
                status="success",
                answer="ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ì˜ ìœ ì‚¬ë„ê°€ ë‚®ì•„ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
                sources=[],
                document_ids=[],
                processing_time=time.time() - start_time
            )

        # 4. OpenAI APIë¡œ ë‹µë³€ ìƒì„±
        context = "\n\n".join(context_chunks[:3])

        system_prompt = """ë‹¹ì‹ ì€ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ì–¸ê¸‰í•˜ì„¸ìš”
5. í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° ê·¸ë ‡ë‹¤ê³  ëª…ì‹œí•˜ì„¸ìš”"""

        user_prompt = f"""ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {request.message}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        try:
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

        except Exception as openai_error:
            print(f"âŒ OpenAI API ì˜¤ë¥˜: {openai_error}")
            # OpenAI API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
            answer = f"ë¬¸ì„œì—ì„œ ë‹¤ìŒê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n{context[:500]}...\n\në” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìœ„í•´ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì •ë¦¬í•´ ì£¼ì„¸ìš”."

        processing_time = time.time() - start_time

        return ChatResponse(
            status="success",
            answer=answer,
            sources=sources,
            document_ids=list(document_ids),
            processing_time=processing_time
        )

    except Exception as e:
        print(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def get_document_info(document_id: str) -> Dict:
    """ë¬¸ì„œ ì •ë³´ ì¡°íšŒ"""
    try:
        response = supabase.from_('documents').select('*').eq('id', document_id).execute()
        if response.data:
            return response.data[0]
        return {}
    except Exception as e:
        print(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}


# ğŸ”¥ ìˆ˜ì •ëœ í•˜ì´ë¼ì´íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@router.post("/highlight")
async def create_highlighted_response(
        document_id: str,
        page_number: int,
        search_text: str
):
    """
    íŠ¹ì • í…ìŠ¤íŠ¸ë¥¼ í•˜ì´ë¼ì´íŠ¸í•œ PDF ìƒì„± (ìºì‹œ í™œìš©)
    """
    try:
        # ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ì§ì ‘ í™œìš©
        try:
            pdf_file_path = get_document_pdf_path(document_id)
        except Exception as path_error:
            print(f"âš ï¸ ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨, ë¬¸ì„œ ì •ë³´ë¡œ fallback: {path_error}")
            # fallback: ë¬¸ì„œ ì •ë³´ ì¡°íšŒ í›„ ë‹¤ìš´ë¡œë“œ
            doc_info = await get_document_info(document_id)
            if not doc_info.get('url'):
                raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            pdf_file_path = ensure_pdf_cache_exists(document_id, doc_info)
            if not pdf_file_path:
                raise HTTPException(status_code=500, detail="PDF íŒŒì¼ì„ ì¤€ë¹„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
        locations = find_text_locations(pdf_file_path, search_text)

        if not locations:
            return {
                "status": "error",
                "message": "í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

        # í•˜ì´ë¼ì´íŠ¸ëœ PDF ìƒì„±
        highlighted_path = create_highlighted_pdf(pdf_file_path, locations)

        # í•˜ì´ë¼ì´íŠ¸ëœ PDFë¥¼ Supabase Storageì— ì—…ë¡œë“œ
        highlighted_url = await upload_highlighted_pdf(highlighted_path, document_id)

        return {
            "status": "success",
            "highlighted_pdf_url": highlighted_url,
            "page_number": locations[0].page_number,
            "bbox": locations[0].bbox,
            "total_highlights": len(locations)
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ í•˜ì´ë¼ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # í•˜ì´ë¼ì´íŠ¸ëœ ì„ì‹œ íŒŒì¼ë§Œ ì •ë¦¬ (ì›ë³¸ ìºì‹œëŠ” ìœ ì§€)
        if 'highlighted_path' in locals() and os.path.exists(highlighted_path):
            os.remove(highlighted_path)


# ğŸ”¥ ìˆ˜ì •ëœ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ì—”ë“œí¬ì¸íŠ¸
@router.get("/page-navigation/{document_id}")
async def get_page_navigation_info(
        document_id: str,
        search_text: str = Query(..., description="ê²€ìƒ‰í•  í…ìŠ¤íŠ¸")
):
    """
    íŠ¹ì • í…ìŠ¤íŠ¸ì˜ í˜ì´ì§€ ìœ„ì¹˜ ì •ë³´ ë°˜í™˜ (PDF ë·°ì–´ìš©, ìºì‹œ í™œìš©)
    """
    try:
        # ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ì§ì ‘ í™œìš©
        try:
            pdf_file_path = get_document_pdf_path(document_id)
        except Exception as path_error:
            print(f"âš ï¸ ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨, ë¬¸ì„œ ì •ë³´ë¡œ fallback: {path_error}")
            # fallback: ë¬¸ì„œ ì •ë³´ ì¡°íšŒ í›„ ë‹¤ìš´ë¡œë“œ
            doc_info = await get_document_info(document_id)
            if not doc_info.get('url'):
                raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            pdf_file_path = ensure_pdf_cache_exists(document_id, doc_info)
            if not pdf_file_path:
                raise HTTPException(status_code=500, detail="PDF íŒŒì¼ì„ ì¤€ë¹„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
        locations = find_text_locations(pdf_file_path, search_text)

        navigation_info = []
        for location in locations:
            navigation_info.append({
                "page_number": location.page_number,
                "bbox": {
                    "x0": location.bbox[0],
                    "y0": location.bbox[1],
                    "x1": location.bbox[2],
                    "y1": location.bbox[3]
                },
                "context": location.context,
                "matched_text": location.text
            })

        return {
            "status": "success",
            "document_id": document_id,
            "search_text": search_text,
            "total_matches": len(navigation_info),
            "locations": navigation_info,
            "primary_page": navigation_info[0]["page_number"] if navigation_info else 1
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ì •ë³´ ìƒì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ğŸ”¥ ë¬¸ì„œ í˜ì´ì§€ ì •ë³´ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.get("/chat/{document_id}/pages")
async def get_document_pages(document_id: str):
    """
    ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜ ì¡°íšŒ
    """
    try:
        doc_info = await get_document_info(document_id)
        if not doc_info:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì´ í˜ì´ì§€ ìˆ˜ ì¡°íšŒ ë˜ëŠ” PDFì—ì„œ ì§ì ‘ ê³„ì‚°
        total_pages = doc_info.get('total_pages', 1)

        return {
            "status": "success",
            "document_id": document_id,
            "total_pages": total_pages,
            "filename": doc_info.get('filename', 'Unknown')
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ í˜ì´ì§€ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def find_chunk_page_number(doc_info: Dict, chunk_text: str) -> int:
    """ì²­í¬ê°€ ìœ„ì¹˜í•œ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° (ìºì‹œëœ PDF íŒŒì¼ í™œìš©)"""
    try:
        document_id = doc_info.get('id')
        if not document_id:
            return 1

        # ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ì§ì ‘ í™œìš©
        try:
            pdf_file_path = get_document_pdf_path(document_id)
        except Exception as path_error:
            print(f"âš ï¸ ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨: {path_error}")
            # fallback: ì›ê²© ë‹¤ìš´ë¡œë“œ
            return await download_and_find_page_number(doc_info, chunk_text)

        # ìºì‹œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
        locations = find_text_locations(pdf_file_path, chunk_text[:100])
        if locations:
            return locations[0].page_number
        return 1

    except Exception as e:
        print(f"í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° ì‹¤íŒ¨: {e}")
        return 1


async def download_and_find_page_number(doc_info: Dict, chunk_text: str) -> int:
    """ìºì‹œ íŒŒì¼ì´ ì—†ì„ ë•Œ ì›ê²©ì—ì„œ ë‹¤ìš´ë¡œë“œí•´ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° (fallback)"""
    try:
        if not doc_info.get('url'):
            return 1

        document_id = doc_info.get('id', 'unknown')

        # PDFFileManagerë¥¼ í†µí•œ ìºì‹œ ê´€ë¦¬
        pdf_file_path = PDFFileManager.get_or_download(document_id, doc_info['url'])

        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
        locations = find_text_locations(pdf_file_path, chunk_text[:100])
        if locations:
            return locations[0].page_number
        return 1

    except Exception as e:
        print(f"ì›ê²© ë‹¤ìš´ë¡œë“œ í›„ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° ì‹¤íŒ¨: {e}")
        return 1


def get_cached_pdf_path(document_id: str) -> str:
    """ë¬¸ì„œ IDë¡œ ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    return PDFFileManager.get_cached_path(document_id)


def ensure_pdf_cache_exists(document_id: str, doc_info: Dict) -> str:
    """PDF ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        if PDFFileManager.is_cached(document_id):
            return PDFFileManager.get_cached_path(document_id)

        # ìºì‹œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•´ì„œ ìƒì„±
        if not doc_info.get('url'):
            return None

        cached_path = PDFFileManager.get_or_download(document_id, doc_info['url'])
        print(f"âœ… PDF ìºì‹œ íŒŒì¼ ìƒì„±: {cached_path}")
        return cached_path

    except Exception as e:
        print(f"âŒ PDF ìºì‹œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        return None


async def upload_highlighted_pdf(file_path: str, document_id: str) -> str:
    """í•˜ì´ë¼ì´íŠ¸ëœ PDFë¥¼ Supabase Storageì— ì—…ë¡œë“œ"""
    try:
        highlighted_filename = f"highlighted_{document_id}_{int(time.time())}.pdf"

        with open(file_path, 'rb') as f:
            file_content = f.read()

        # Supabase Storageì— ì—…ë¡œë“œ
        upload_response = supabase.storage.from_("pdf-documents").upload(
            path=f"highlighted/{highlighted_filename}",
            file=file_content,
            file_options={"content-type": "application/pdf", "upsert": True}
        )

        # ê³µê°œ URL ìƒì„±
        public_url = supabase.storage.from_("pdf-documents").get_public_url(
            f"highlighted/{highlighted_filename}"
        ).data.get("publicUrl")

        return public_url

    except Exception as e:
        print(f"í•˜ì´ë¼ì´íŠ¸ëœ PDF ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return ""


@router.post("/chat/search")
async def search_documents(query: str, document_ids: List[str] = None, limit: int = 10):
    """
    ë¬¸ì„œ ê²€ìƒ‰ (ë‹µë³€ ìƒì„± ì—†ì´ ê²€ìƒ‰ë§Œ, í˜ì´ì§€ ì •ë³´ í¬í•¨, ìºì‹œ í™œìš©)
    """
    try:
        # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = get_embeddings([query])[0]

        # ê²€ìƒ‰ ì‹¤í–‰
        search_kwargs = {
            "query_embeddings": [query_embedding],
            "n_results": limit
        }

        if document_ids:
            search_kwargs["where"] = {
                "document_id": {"$in": document_ids}
            }

        search_results = vector_collection.query(**search_kwargs)

        if not search_results["documents"] or not search_results["documents"][0]:
            return {
                "status": "success",
                "results": [],
                "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        # ê²°ê³¼ ì •ë¦¬ (í˜ì´ì§€ ì •ë³´ í¬í•¨)
        documents = search_results["documents"][0]
        metadatas = search_results["metadatas"][0]
        distances = search_results["distances"][0]

        results = []
        for doc, metadata, distance in zip(documents, metadatas, distances):
            document_id = metadata.get("document_id", "")
            start_page = metadata.get("start_page", 1)
            end_page = metadata.get("end_page", 1)

            results.append({
                "document_id": document_id,
                "filename": metadata.get("filename", "Unknown"),
                "chunk_index": metadata.get("chunk_index", 0),
                "content": doc,
                "relevance_score": 1 - distance,
                "distance": distance,
                "page_number": start_page,
                "end_page": end_page,
                "chunk_hash": metadata.get("chunk_hash", ""),
                "char_count": metadata.get("char_count", len(doc)),
                "created_at": metadata.get("created_at", ""),
                "navigation_info": {
                    "can_highlight": True,
                    "can_navigate": True,
                    "is_cached": PDFFileManager.is_cached(document_id)
                }
            })

        return {
            "status": "success",
            "results": results,
            "count": len(results)
        }

    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/chat/documents")
async def get_available_documents():
    """
    ì±„íŒ…ì— ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (ì™„ë£Œëœ ë¬¸ì„œë§Œ, ìºì‹œ ìƒíƒœ í¬í•¨)
    """
    try:
        response = supabase.from_('documents').select('id, filename, created_at, processed_at').eq('status',
                                                                                                   'completed').order(
            'created_at', desc=True).execute()

        # ìºì‹œ ìƒíƒœ ì •ë³´ ì¶”ê°€
        documents_with_cache_info = []
        for doc in response.data:
            doc_with_cache = doc.copy()
            doc_with_cache['is_cached'] = PDFFileManager.is_cached(doc['id'])
            documents_with_cache_info.append(doc_with_cache)

        return {
            "status": "success",
            "documents": documents_with_cache_info,
            "count": len(documents_with_cache_info)
        }

    except Exception as e:
        print(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/feedback")
async def submit_chat_feedback(
        message: str,
        answer: str,
        rating: int,
        feedback_text: str = None,
        page_navigation_used: bool = False,
        highlight_used: bool = False
):
    """
    ì±„íŒ… ë‹µë³€ì— ëŒ€í•œ í”¼ë“œë°± ìˆ˜ì§‘ (í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜/í•˜ì´ë¼ì´íŠ¸ ì‚¬ìš© ì—¬ë¶€ í¬í•¨)
    """
    try:
        feedback_data = {
            "message": message,
            "answer": answer,
            "rating": rating,
            "feedback_text": feedback_text,
            "page_navigation_used": page_navigation_used,
            "highlight_used": highlight_used,
            "timestamp": time.time()
        }

        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ë¡œê·¸ë§Œ ì¶œë ¥ (ì‹¤ì œë¡œëŠ” DBì— ì €ì¥)
        print(f"ğŸ“ ì‚¬ìš©ì í”¼ë“œë°±: {feedback_data}")

        return {
            "status": "success",
            "message": "í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        print(f"âŒ í”¼ë“œë°± ì €ì¥ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/chat/cache-status")
async def get_cache_status():
    """
    PDF ìºì‹œ ìƒíƒœ ì¡°íšŒ (ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ìš©)
    """
    try:
        cache_info = []

        # ì™„ë£Œëœ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
        response = supabase.from_('documents').select('id, filename, status').eq('status', 'completed').execute()

        for doc in response.data:
            document_id = doc['id']
            is_cached = PDFFileManager.is_cached(document_id)
            cache_path = PDFFileManager.get_cached_path(document_id)

            file_size = 0
            if is_cached and os.path.exists(cache_path):
                file_size = os.path.getsize(cache_path)

            cache_info.append({
                "document_id": document_id,
                "filename": doc['filename'],
                "is_cached": is_cached,
                "cache_path": cache_path if is_cached else None,
                "file_size_bytes": file_size,
                "file_size_mb": round(file_size / (1024 * 1024), 2) if file_size > 0 else 0
            })

        total_cached = sum(1 for info in cache_info if info['is_cached'])
        total_size_mb = sum(info['file_size_mb'] for info in cache_info)

        return {
            "status": "success",
            "summary": {
                "total_documents": len(cache_info),
                "total_cached": total_cached,
                "cache_hit_rate": round(total_cached / len(cache_info) * 100, 1) if cache_info else 0,
                "total_cache_size_mb": round(total_size_mb, 2)
            },
            "cache_details": cache_info
        }

    except Exception as e:
        print(f"âŒ ìºì‹œ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/clear-cache")
async def clear_document_cache(document_id: str = None):
    """
    PDF ìºì‹œ ì •ë¦¬ (íŠ¹ì • ë¬¸ì„œ ë˜ëŠ” ì „ì²´)
    """
    try:
        if document_id:
            # íŠ¹ì • ë¬¸ì„œ ìºì‹œ ì‚­ì œ
            if PDFFileManager.is_cached(document_id):
                PDFFileManager.cleanup_cache(document_id)
                return {
                    "status": "success",
                    "message": f"ë¬¸ì„œ {document_id}ì˜ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            else:
                return {
                    "status": "success",
                    "message": f"ë¬¸ì„œ {document_id}ì˜ ìºì‹œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                }
        else:
            # ì „ì²´ ìºì‹œ ì •ë¦¬ëŠ” ë³´ì•ˆìƒ ì œí•œì ìœ¼ë¡œ êµ¬í˜„
            return {
                "status": "error",
                "message": "ì „ì²´ ìºì‹œ ì‚­ì œëŠ” ì‹œìŠ¤í…œ ê´€ë¦¬ìë§Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        print(f"âŒ ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))