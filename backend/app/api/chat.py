# backend/app/api/chat.py
from fastapi import APIRouter, HTTPException
from app.models.schemas import ChatRequest, ChatResponse
from app.services.document_processor import vector_collection
from app.services.vectorizer import get_embeddings
from app.services.supabase_client import supabase
import openai
import time
from typing import List, Dict, Any
import traceback

router = APIRouter()


@router.post("/chat", response_model=ChatResponse)
async def chat_with_documents(request: ChatRequest):
    """
    ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
    """
    start_time = time.time()

    try:
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = get_embeddings([request.message])[0]

        # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰
        search_kwargs = {
            "query_embeddings": [query_embedding],
            "n_results": request.max_results
        }

        # íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
        if request.document_ids:
            search_kwargs["where"] = {
                "document_id": {"$in": request.document_ids}
            }

        search_results = vector_collection.query(**search_kwargs)

        if not search_results["documents"] or not search_results["documents"][0]:
            return ChatResponse(
                status="success",
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                sources=[],
                document_ids=[],
                processing_time=time.time() - start_time
            )

        # 3. ê²€ìƒ‰ëœ ì²­í¬ë“¤ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        documents = search_results["documents"][0]
        metadatas = search_results["metadatas"][0]
        distances = search_results["distances"][0]

        context_chunks = []
        sources = []
        document_ids = set()

        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
            if distance < 0.8:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                context_chunks.append(doc)
                document_ids.add(metadata.get("document_id", ""))

                sources.append({
                    "document_id": metadata.get("document_id", ""),
                    "filename": metadata.get("filename", "Unknown"),
                    "chunk_index": metadata.get("chunk_index", i),
                    "relevance_score": 1 - distance,  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    "content_preview": doc[:200] + "..." if len(doc) > 200 else doc
                })

        if not context_chunks:
            return ChatResponse(
                status="success",
                answer="ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ì˜ ìœ ì‚¬ë„ê°€ ë‚®ì•„ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
                sources=[],
                document_ids=[],
                processing_time=time.time() - start_time
            )

        # 4. OpenAI APIë¡œ ë‹µë³€ ìƒì„±
        context = "\n\n".join(context_chunks[:3])  # ìƒìœ„ 3ê°œ ì²­í¬ë§Œ ì‚¬ìš©

        system_prompt = """ë‹¹ì‹ ì€ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ì–¸ê¸‰í•˜ì„¸ìš”
5. í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° ê·¸ë ‡ë‹¤ê³  ëª…ì‹œí•˜ì„¸ìš”"""

        user_prompt = f"""ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {request.message}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        try:
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

        except Exception as openai_error:
            print(f"âŒ OpenAI API ì˜¤ë¥˜: {openai_error}")
            # OpenAI API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
            answer = f"ë¬¸ì„œì—ì„œ ë‹¤ìŒê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n{context[:500]}...\n\në” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìœ„í•´ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì •ë¦¬í•´ ì£¼ì„¸ìš”."

        processing_time = time.time() - start_time

        return ChatResponse(
            status="success",
            answer=answer,
            sources=sources,
            document_ids=list(document_ids),
            processing_time=processing_time
        )

    except Exception as e:
        print(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.post("/chat/search")
async def search_documents(query: str, document_ids: List[str] = None, limit: int = 10):
    """
    ë¬¸ì„œ ê²€ìƒ‰ (ë‹µë³€ ìƒì„± ì—†ì´ ê²€ìƒ‰ë§Œ)
    """
    try:
        # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = get_embeddings([query])[0]

        # ê²€ìƒ‰ ì‹¤í–‰
        search_kwargs = {
            "query_embeddings": [query_embedding],
            "n_results": limit
        }

        if document_ids:
            search_kwargs["where"] = {
                "document_id": {"$in": document_ids}
            }

        search_results = vector_collection.query(**search_kwargs)

        if not search_results["documents"] or not search_results["documents"][0]:
            return {
                "status": "success",
                "results": [],
                "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        # ê²°ê³¼ ì •ë¦¬
        documents = search_results["documents"][0]
        metadatas = search_results["metadatas"][0]
        distances = search_results["distances"][0]

        results = []
        for doc, metadata, distance in zip(documents, metadatas, distances):
            results.append({
                "document_id": metadata.get("document_id", ""),
                "filename": metadata.get("filename", "Unknown"),
                "chunk_index": metadata.get("chunk_index", 0),
                "content": doc,
                "relevance_score": 1 - distance,
                "distance": distance
            })

        return {
            "status": "success",
            "results": results,
            "count": len(results)
        }

    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/chat/documents")
async def get_available_documents():
    """
    ì±„íŒ…ì— ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (ì™„ë£Œëœ ë¬¸ì„œë§Œ)
    """
    try:
        response = supabase.from_('documents').select('id, filename, created_at, processed_at').eq('status',
                                                                                                   'completed').order(
            'created_at', desc=True).execute()

        return {
            "status": "success",
            "documents": response.data,
            "count": len(response.data)
        }

    except Exception as e:
        print(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/feedback")
async def submit_chat_feedback(
        message: str,
        answer: str,
        rating: int,
        feedback_text: str = None
):
    """
    ì±„íŒ… ë‹µë³€ì— ëŒ€í•œ í”¼ë“œë°± ìˆ˜ì§‘
    """
    try:
        # í”¼ë“œë°±ì„ ë¡œê·¸ë‚˜ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥í•  ìˆ˜ ìˆìŒ
        feedback_data = {
            "message": message,
            "answer": answer,
            "rating": rating,
            "feedback_text": feedback_text,
            "timestamp": time.time()
        }

        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ë¡œê·¸ë§Œ ì¶œë ¥
        print(f"ğŸ“ ì‚¬ìš©ì í”¼ë“œë°±: {feedback_data}")

        return {
            "status": "success",
            "message": "í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        print(f"âŒ í”¼ë“œë°± ì €ì¥ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))