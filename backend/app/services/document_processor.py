# backend/app/services/document_processor.py (Enhanced for RAG with Location)
import requests
import os
import chromadb
from app.services.supabase_client import supabase
from app.services.pdf_service import process_pdf_file_with_locations, find_text_locations
from app.services.vectorizer import get_embeddings
from datetime import datetime
from typing import List, Dict
import traceback
import json
import tempfile
import shutil
from pathlib import Path

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection_name = "pdf_document_embeddings"

try:
    vector_collection = chroma_client.get_collection(name=collection_name)
    print(f"ChromaDB collection '{collection_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
except Exception:
    vector_collection = chroma_client.create_collection(name=collection_name)
    print(f"ChromaDB collection '{collection_name}'ì„ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

# ğŸ”¥ PDF íŒŒì¼ ìºì‹œ ê´€ë¦¬
PDF_CACHE_DIR = "./pdf_cache"
os.makedirs(PDF_CACHE_DIR, exist_ok=True)


class PDFFileManager:
    """PDF íŒŒì¼ ìƒëª…ì£¼ê¸° ê´€ë¦¬ í´ë˜ìŠ¤"""

    @staticmethod
    def get_cached_path(document_id: str) -> str:
        """ìºì‹œëœ PDF íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        return os.path.join(PDF_CACHE_DIR, f"{document_id}.pdf")

    @staticmethod
    def is_cached(document_id: str) -> bool:
        """PDF íŒŒì¼ì´ ìºì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        cached_path = PDFFileManager.get_cached_path(document_id)
        return os.path.exists(cached_path) and os.path.getsize(cached_path) > 0

    @staticmethod
    def save_to_cache(document_id: str, file_content: bytes) -> str:
        """PDF íŒŒì¼ì„ ìºì‹œì— ì €ì¥"""
        cached_path = PDFFileManager.get_cached_path(document_id)
        with open(cached_path, 'wb') as f:
            f.write(file_content)
        return cached_path

    @staticmethod
    def get_or_download(document_id: str, file_url: str) -> str:
        """ìºì‹œì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ"""
        cached_path = PDFFileManager.get_cached_path(document_id)

        # ìºì‹œì— ìˆìœ¼ë©´ ë°˜í™˜
        if PDFFileManager.is_cached(document_id):
            print(f"[{document_id}] ìºì‹œëœ íŒŒì¼ ì‚¬ìš©: {cached_path}")
            return cached_path

        # ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œì— ì €ì¥
        print(f"[{document_id}] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ì €ì¥...")
        file_content = PDFFileManager._download_file(file_url, document_id)
        return PDFFileManager.save_to_cache(document_id, file_content)

    @staticmethod
    def _download_file(file_url: str, document_id: str) -> bytes:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Supabase Storage ë˜ëŠ” ê³µê°œ URL)"""
        bucket_name = "pdf-documents"

        try:
            # Supabase Storageì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„
            if f"/{bucket_name}/" in file_url:
                path_in_storage = file_url.split(f"/{bucket_name}/")[-1]
            else:
                path_in_storage = file_url.split("/")[-1]

            print(f"[{document_id}] Supabase Storageì—ì„œ ë‹¤ìš´ë¡œë“œ: {path_in_storage}")
            file_bytes = supabase.storage.from_(bucket_name).download(path_in_storage)

            if file_bytes and isinstance(file_bytes, bytes) and len(file_bytes) > 0:
                print(f"[{document_id}] Supabase Storage ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {len(file_bytes)} bytes")
                return file_bytes
            else:
                raise Exception("Supabase Storageì—ì„œ ë¹ˆ íŒŒì¼ ë˜ëŠ” ì˜ëª»ëœ ì‘ë‹µ")

        except Exception as storage_error:
            print(f"[{document_id}] Supabase Storage ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {storage_error}")

            # ê³µê°œ URLë¡œ ëŒ€ì•ˆ ë‹¤ìš´ë¡œë“œ
            try:
                print(f"[{document_id}] ê³µê°œ URLë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                response = requests.get(file_url, timeout=30)
                response.raise_for_status()

                if len(response.content) == 0:
                    raise Exception("ê³µê°œ URLì—ì„œ ë¹ˆ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¨")

                print(f"[{document_id}] ê³µê°œ URL ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {len(response.content)} bytes")
                return response.content

            except Exception as url_error:
                raise Exception(f"ëª¨ë“  ë‹¤ìš´ë¡œë“œ ë°©ë²• ì‹¤íŒ¨ - Storage: {storage_error}, URL: {url_error}")

    @staticmethod
    def cleanup_cache(document_id: str = None):
        """ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        if document_id:
            # íŠ¹ì • ë¬¸ì„œ ìºì‹œ ì‚­ì œ
            cached_path = PDFFileManager.get_cached_path(document_id)
            if os.path.exists(cached_path):
                os.remove(cached_path)
                print(f"[{document_id}] ìºì‹œ íŒŒì¼ ì‚­ì œ: {cached_path}")
        else:
            # ì „ì²´ ìºì‹œ ì •ë¦¬ (ì„ íƒì )
            pass


async def download_pdf_from_supabase(file_url: str, document_id: str) -> str:
    """
    ğŸ”¥ ê°œì„ ëœ PDF ë‹¤ìš´ë¡œë“œ (ìºì‹œ í™œìš©)
    """
    return PDFFileManager.get_or_download(document_id, file_url)


async def process_single_document(document_id: str, filename: str, file_url: str) -> Dict:
    """
    ğŸ”¥ ê°œì„ ëœ ë‹¨ì¼ PDF ë¬¸ì„œ ì²˜ë¦¬ (íŒŒì¼ ìƒëª…ì£¼ê¸° ê´€ë¦¬ ê°œì„ )
    """
    pdf_file_path = None

    try:
        print(f"[{document_id}] ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {filename}")

        # 1. PDF íŒŒì¼ íšë“ (ìºì‹œ ìš°ì„  í™œìš©)
        print(f"[{document_id}] PDF íŒŒì¼ íšë“ ì‹œì‘: {file_url}")
        pdf_file_path = await download_pdf_from_supabase(file_url, document_id)

        # 2. íŒŒì¼ ìœ íš¨ì„± ê²€ì¦
        if not os.path.exists(pdf_file_path):
            raise Exception(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file_path}")

        file_size = os.path.getsize(pdf_file_path)
        if file_size == 0:
            raise Exception("PDF íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        print(f"[{document_id}] PDF íŒŒì¼ í™•ì¸ë¨: {pdf_file_path} ({file_size} bytes)")

        # 3. PDF ìœ„ì¹˜ ì •ë³´ í¬í•¨ ë²¡í„°í™” ì²˜ë¦¬
        print(f"[{document_id}] PDF ìœ„ì¹˜ ì •ë³´ í¬í•¨ ë²¡í„°í™” ì²˜ë¦¬ ì‹œì‘...")
        vectorization_result = process_pdf_file_with_locations(pdf_file_path)

        if vectorization_result["status"] != "success":
            raise Exception(f"PDF ë²¡í„°í™” ì²˜ë¦¬ ì‹¤íŒ¨: {vectorization_result.get('error', 'Unknown error')}")

        chunks = vectorization_result["chunks"]
        chunks_with_locations = vectorization_result["chunks_with_locations"]

        if not chunks:
            raise Exception("PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # 4. ì„ë² ë”© ìƒì„±
        print(f"[{document_id}] ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = get_embeddings(chunks)

        if len(chunks) != len(embeddings):
            raise Exception(f"ì²­í¬ ìˆ˜({len(chunks)})ì™€ ì„ë² ë”© ìˆ˜({len(embeddings)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        # 5. ChromaDBì— ì²­í¬ì™€ ì„ë² ë”© ì €ì¥ (ìœ„ì¹˜ ì •ë³´ í¬í•¨)
        print(f"[{document_id}] ChromaDBì— {len(chunks)}ê°œ ì„ë² ë”© ì €ì¥ ì¤‘...")

        ids = [f"{document_id}_chunk_{i}" for i in range(len(chunks))]
        metadatas = []

        for i, chunk_location in enumerate(chunks_with_locations):
            metadata = {
                "document_id": document_id,
                "filename": filename,
                "chunk_index": i,
                "start_page": chunk_location["start_page"],
                "end_page": chunk_location["end_page"],
                "chunk_hash": chunk_location["chunk_hash"],
                "char_count": chunk_location["char_count"],
                "created_at": datetime.now().isoformat(),
                "pdf_file_path": pdf_file_path  # ğŸ”¥ íŒŒì¼ ê²½ë¡œ ì €ì¥
            }
            metadatas.append(metadata)

        # ChromaDBì— ì €ì¥ (ê¸°ì¡´ IDê°€ ìˆë‹¤ë©´ ì‚­ì œ í›„ ì¶”ê°€)
        try:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë° ì‚­ì œ
            existing_ids = []
            try:
                existing_data = vector_collection.get(
                    where={"document_id": document_id}
                )
                if existing_data["ids"]:
                    existing_ids = existing_data["ids"]
                    print(f"[{document_id}] ê¸°ì¡´ {len(existing_ids)}ê°œ ì²­í¬ ë°œê²¬, ì‚­ì œ í›„ ì¬ì¶”ê°€")
                    vector_collection.delete(ids=existing_ids)
            except Exception as get_error:
                print(f"[{document_id}] ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {get_error}")

            # ìƒˆ ë°ì´í„° ì¶”ê°€
            vector_collection.add(
                embeddings=embeddings,
                documents=chunks,
                metadatas=metadatas,
                ids=ids
            )
            print(f"[{document_id}] {len(chunks)}ê°œì˜ ì²­í¬ê°€ ChromaDBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as chroma_error:
            raise Exception(f"ChromaDB ì €ì¥ ì‹¤íŒ¨: {chroma_error}")

        # 6. Supabase DBì˜ ë¬¸ì„œ ìƒíƒœë¥¼ 'completed'ë¡œ ì—…ë°ì´íŠ¸
        print(f"[{document_id}] Supabase DB ìƒíƒœ 'completed'ë¡œ ì—…ë°ì´íŠ¸ ì¤‘...")

        try:
            update_response = supabase.from_('documents').update({
                'status': 'completed',
                'processed_at': datetime.now().isoformat(),
            }).eq('id', document_id).execute()

            print(f"[{document_id}] DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì„±ê³µ")

        except Exception as db_error:
            raise Exception(f"Supabase DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {db_error}")

        print(f"[{document_id}] âœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ. ìƒíƒœ: completed")
        return {
            "status": "success",
            "message": f"ë¬¸ì„œ '{filename}' ì²˜ë¦¬ ì™„ë£Œ. {len(chunks)}ê°œ ì²­í¬ ì €ì¥ë¨.",
            "chunk_count": len(chunks),
            "document_id": document_id,
            "pdf_file_path": pdf_file_path,  # ğŸ”¥ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (í•˜ì´ë¼ì´íŠ¸ìš©)
            "location_info": {
                "total_pages": vectorization_result.get("text_extraction", {}).get("total_pages", 0),
                "chunks_with_pages": chunks_with_locations
            }
        }

    except Exception as e:
        error_msg = str(e)
        print(f"[{document_id}] âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        traceback.print_exc()

        # ì˜¤ë¥˜ ë°œìƒ ì‹œ Supabase DB ìƒíƒœë¥¼ 'failed'ë¡œ ì—…ë°ì´íŠ¸
        try:
            print(f"[{document_id}] DB ìƒíƒœë¥¼ 'failed'ë¡œ ì—…ë°ì´íŠ¸ ì¤‘...")
            update_response = supabase.from_('documents').update({
                'status': 'failed',
                'processed_at': datetime.now().isoformat(),
            }).eq('id', document_id).execute()

            print(f"[{document_id}] ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

        except Exception as db_update_err:
            print(f"[{document_id}] âš ï¸ ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {db_update_err}")

        return {
            "status": "failed",
            "message": f"ë¬¸ì„œ '{filename}' ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}",
            "document_id": document_id
        }

    # ğŸ”¥ ì„ì‹œ íŒŒì¼ ì‚­ì œí•˜ì§€ ì•ŠìŒ - ìºì‹œë¡œ ìœ ì§€í•˜ì—¬ í›„ì† ì‘ì—…ì—ì„œ í™œìš©


def get_document_pdf_path(document_id: str) -> str:
    """
    ğŸ”¥ ë¬¸ì„œ IDë¡œ PDF íŒŒì¼ ê²½ë¡œ ì¡°íšŒ (ìºì‹œ ìš°ì„ )
    """
    # 1. ìºì‹œì—ì„œ í™•ì¸
    if PDFFileManager.is_cached(document_id):
        return PDFFileManager.get_cached_path(document_id)

    # 2. ChromaDB ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸
    try:
        results = vector_collection.get(
            where={"document_id": document_id},
            limit=1
        )

        if results["metadatas"] and results["metadatas"][0]:
            stored_path = results["metadatas"][0].get("pdf_file_path")
            if stored_path and os.path.exists(stored_path):
                return stored_path
    except Exception as e:
        print(f"ChromaDBì—ì„œ íŒŒì¼ ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # 3. Supabaseì—ì„œ URL ì¡°íšŒ í›„ ë‹¤ìš´ë¡œë“œ
    try:
        response = supabase.from_('documents').select('url').eq('id', document_id).execute()
        if response.data:
            file_url = response.data[0]['url']
            return PDFFileManager.get_or_download(document_id, file_url)
    except Exception as e:
        print(f"Supabaseì—ì„œ íŒŒì¼ URL ì¡°íšŒ ì‹¤íŒ¨: {e}")

    raise Exception(f"ë¬¸ì„œ {document_id}ì˜ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")


def find_chunk_exact_location(document_id: str, chunk_text: str, file_url: str = None) -> Dict:
    """
    ğŸ”¥ ê°œì„ ëœ ì²­í¬ ìœ„ì¹˜ ì°¾ê¸° (ìºì‹œëœ íŒŒì¼ í™œìš©)
    """
    try:
        # PDF íŒŒì¼ ê²½ë¡œ íšë“
        pdf_file_path = get_document_pdf_path(document_id)

        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
        search_text = chunk_text[:200]  # ì²­í¬ì˜ ì²« 200ìë¡œ ê²€ìƒ‰
        locations = find_text_locations(pdf_file_path, search_text)

        if locations:
            primary_location = locations[0]
            return {
                "status": "success",
                "page_number": primary_location.page_number,
                "bbox": {
                    "x0": primary_location.bbox[0],
                    "y0": primary_location.bbox[1],
                    "x1": primary_location.bbox[2],
                    "y1": primary_location.bbox[3]
                },
                "context": primary_location.context,
                "total_matches": len(locations),
                "pdf_file_path": pdf_file_path
            }
        else:
            return {
                "status": "not_found",
                "message": "í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ PDFì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }


def search_similar_documents_with_pages(query_text: str, limit: int = 5, document_ids: List[str] = None) -> Dict:
    """
    ğŸ”¥ ê°œì„ ëœ ë¬¸ì„œ ê²€ìƒ‰ (PDF íŒŒì¼ ê²½ë¡œ í¬í•¨)
    """
    try:
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = get_embeddings([query_text])[0]

        # ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •
        search_kwargs = {
            "query_embeddings": [query_embedding],
            "n_results": limit
        }

        # íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
        if document_ids:
            search_kwargs["where"] = {
                "document_id": {"$in": document_ids}
            }

        # ChromaDBì—ì„œ ê²€ìƒ‰
        results = vector_collection.query(**search_kwargs)

        if not results["documents"] or not results["documents"][0]:
            return {
                "status": "success",
                "results": [],
                "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        # ê²°ê³¼ ì •ë¦¬ (í˜ì´ì§€ ì •ë³´ í¬í•¨)
        documents = results["documents"][0]
        metadatas = results["metadatas"][0]
        distances = results["distances"][0]

        search_results = []
        for doc, metadata, distance in zip(documents, metadatas, distances):
            # ğŸ”¥ PDF íŒŒì¼ ê²½ë¡œ í™•ì¸
            try:
                pdf_path = get_document_pdf_path(metadata.get("document_id", ""))
            except:
                pdf_path = None

            result = {
                "document_id": metadata.get("document_id", ""),
                "filename": metadata.get("filename", "Unknown"),
                "chunk_index": metadata.get("chunk_index", 0),
                "content": doc,
                "relevance_score": 1 - distance,
                "distance": distance,
                "page_number": metadata.get("start_page", 1),
                "end_page": metadata.get("end_page", 1),
                "chunk_hash": metadata.get("chunk_hash", ""),
                "char_count": metadata.get("char_count", len(doc)),
                "created_at": metadata.get("created_at", ""),
                "pdf_file_path": pdf_path  # ğŸ”¥ PDF íŒŒì¼ ê²½ë¡œ ì¶”ê°€
            }
            search_results.append(result)

        return {
            "status": "success",
            "results": search_results,
            "query": query_text,
            "total_results": len(search_results)
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}


# ğŸ”¥ ë‚˜ë¨¸ì§€ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€...
def get_document_status(document_id: str) -> Dict:
    """íŠ¹ì • ë¬¸ì„œì˜ ì²˜ë¦¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        response = supabase.from_('documents').select('*').eq('id', document_id).execute()
        if response.data:
            return {"status": "success", "data": response.data[0]}
        else:
            return {"status": "not_found", "message": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


def get_documents_by_status(status: str = None, limit: int = 10) -> Dict:
    """ìƒíƒœë³„ë¡œ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        query = supabase.from_('documents').select('*').order('created_at', desc=True).limit(limit)
        if status:
            query = query.eq('status', status)
        response = query.execute()
        return {
            "status": "success",
            "data": response.data,
            "count": len(response.data)
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


def delete_document_vectors(document_id: str) -> Dict:
    """
    ğŸ”¥ ê°œì„ ëœ ë¬¸ì„œ ë²¡í„° ì‚­ì œ (ìºì‹œë„ í•¨ê»˜ ì •ë¦¬)
    """
    try:
        # ChromaDBì—ì„œ ë²¡í„° ë°ì´í„° ì‚­ì œ
        results = vector_collection.get(where={"document_id": document_id})

        if not results["ids"]:
            return {
                "status": "not_found",
                "message": "ì‚­ì œí•  ë²¡í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        vector_collection.delete(ids=results["ids"])

        # ìºì‹œ íŒŒì¼ë„ ì‚­ì œ
        PDFFileManager.cleanup_cache(document_id)

        return {
            "status": "success",
            "message": f"{len(results['ids'])}ê°œì˜ ë²¡í„° ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": len(results["ids"])
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ë“¤
def search_similar_documents(query_text: str, limit: int = 5) -> Dict:
    """ê¸°ì¡´ ê²€ìƒ‰ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return search_similar_documents_with_pages(query_text, limit)


def get_document_page_info(document_id: str) -> Dict:
    """ë¬¸ì„œì˜ í˜ì´ì§€ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        results = vector_collection.get(where={"document_id": document_id})

        if not results["ids"]:
            return {"status": "not_found", "message": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        page_info = {}
        total_chunks = len(results["ids"])

        for metadata in results["metadatas"]:
            start_page = metadata.get("start_page", 1)
            end_page = metadata.get("end_page", 1)

            for page in range(start_page, end_page + 1):
                if page not in page_info:
                    page_info[page] = {
                        "page_number": page,
                        "chunk_count": 0,
                        "chunks": []
                    }

                page_info[page]["chunk_count"] += 1
                page_info[page]["chunks"].append({
                    "chunk_index": metadata.get("chunk_index", 0),
                    "chunk_hash": metadata.get("chunk_hash", ""),
                    "char_count": metadata.get("char_count", 0)
                })

        return {
            "status": "success",
            "document_id": document_id,
            "total_chunks": total_chunks,
            "total_pages": len(page_info),
            "page_info": list(page_info.values())
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}


async def reprocess_failed_documents() -> Dict:
    """ì‹¤íŒ¨í•œ ë¬¸ì„œë“¤ì„ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        response = supabase.from_('documents').select('*').eq('status', 'failed').execute()
        failed_docs = response.data

        if not failed_docs:
            return {"status": "success", "message": "ì¬ì²˜ë¦¬í•  ì‹¤íŒ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤", "count": 0}

        success_count = 0
        results = []

        for doc in failed_docs:
            print(f"ì¬ì²˜ë¦¬ ì‹œì‘: {doc['filename']} (ID: {doc['id']})")

            supabase.from_('documents').update({
                'status': 'pending',
                'processed_at': None
            }).eq('id', doc['id']).execute()

            result = await process_single_document(doc['id'], doc['filename'], doc['url'])
            results.append({
                "document_id": doc['id'],
                "filename": doc['filename'],
                "result": result
            })

            if result['status'] == 'success':
                success_count += 1

        return {
            "status": "success",
            "message": f"{len(failed_docs)}ê°œ ë¬¸ì„œ ì¬ì²˜ë¦¬ ì™„ë£Œ. ì„±ê³µ: {success_count}ê°œ",
            "total_processed": len(failed_docs),
            "success_count": success_count,
            "results": results
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}


def get_chunk_by_hash(chunk_hash: str) -> Dict:
    """ì²­í¬ í•´ì‹œë¡œ íŠ¹ì • ì²­í¬ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        results = vector_collection.get(where={"chunk_hash": chunk_hash})

        if not results["ids"]:
            return {"status": "not_found", "message": "í•´ë‹¹ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        chunk_data = {
            "chunk_id": results["ids"][0],
            "document": results["documents"][0],
            "metadata": results["metadatas"][0]
        }

        return {"status": "success", "chunk": chunk_data}

    except Exception as e:
        return {"status": "error", "message": str(e)}


if __name__ == "__main__":
    print("âœ¨ Enhanced Document processor ëª¨ë“ˆì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ğŸ”¥ ê°œì„ ì‚¬í•­:")
    print("  - PDF íŒŒì¼ ìºì‹œ ê´€ë¦¬")
    print("  - íŒŒì¼ ìƒëª…ì£¼ê¸° ìµœì í™”")
    print("  - ê²½ë¡œ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°")
    print("  - í˜ì´ì§€ ìœ„ì¹˜ ì •ë³´ ë° í•˜ì´ë¼ì´íŠ¸ ê¸°ëŠ¥ ì§€ì›")