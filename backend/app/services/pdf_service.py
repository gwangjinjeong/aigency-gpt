# backend/app/services/pdf_service.py
import fitz  # PyMuPDF
import os
import tempfile
import requests
import re
import json
import time
from typing import List, Dict, Optional, Tuple, NamedTuple
from datetime import datetime
import logging
from pathlib import Path
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextLocation(NamedTuple):
    """í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì •ë³´"""
    page_number: int
    bbox: Tuple[float, float, float, float]  # (x0, y0, x1, y1)
    text: str
    context: str  # ì•ë’¤ ë¬¸ë§¥


class ChunkLocation(NamedTuple):
    """ì²­í¬ ìœ„ì¹˜ ì •ë³´"""
    chunk_text: str
    page_locations: List[TextLocation]
    start_page: int
    end_page: int
    chunk_hash: str


class PDFProcessor:
    """PDF íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ (RAG/RAGAS ëŒ€ì‘)"""

    def __init__(self):
        self.supported_formats = ['.pdf']
        self.location_cache = {}  # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ìºì‹œ

    def normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (í•œê¸€ ê²€ìƒ‰ ê°œì„ )"""
        if not text:
            return ""

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        text = re.sub(r'\s+', ' ', text)

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def validate_pdf(self, file_path: str) -> bool:
        """PDF íŒŒì¼ì˜ ìœ íš¨ì„±ì„ ê²€ì¦"""
        doc = None
        try:
            doc = fitz.open(file_path)
            page_count = len(doc)

            if page_count == 0:
                logger.error(f"PDF íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {file_path}")
                return False

            logger.info(f"PDF ê²€ì¦ ì„±ê³µ: {page_count}í˜ì´ì§€")
            return True

        except Exception as e:
            logger.error(f"PDF ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
        finally:
            if doc is not None:
                try:
                    doc.close()
                except:
                    pass

    def extract_text_with_locations(self, file_path: str) -> Dict:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìœ„ì¹˜ ì •ë³´ì™€ í•¨ê»˜ ì¶”ì¶œ"""
        doc = None
        try:
            doc = fitz.open(file_path)
            text_content = []
            page_locations = {}  # page_num -> [TextLocation]
            total_pages = len(doc)  # ë¬¸ì„œê°€ ì—´ë¦° ìƒíƒœì—ì„œ í˜ì´ì§€ ìˆ˜ ì €ì¥

            for page_num in range(total_pages):
                page = doc.load_page(page_num)

                # í…ìŠ¤íŠ¸ ë¸”ë¡ë³„ë¡œ ì¶”ì¶œ (ìœ„ì¹˜ ì •ë³´ í¬í•¨)
                blocks = page.get_text("dict")["blocks"]
                page_text_locations = []
                page_full_text = ""

                for block in blocks:
                    if "lines" in block:  # í…ìŠ¤íŠ¸ ë¸”ë¡ì¸ ê²½ìš°
                        for line in block["lines"]:
                            for span in line["spans"]:
                                text = self.normalize_text(span["text"])  # ğŸ”¥ ì •ê·œí™” ì¶”ê°€
                                if text:
                                    bbox = span["bbox"]  # (x0, y0, x1, y1)

                                    # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì •ë³´ ì €ì¥
                                    text_location = TextLocation(
                                        page_number=page_num + 1,
                                        bbox=bbox,
                                        text=text,
                                        context=""  # ë‚˜ì¤‘ì— ì„¤ì •
                                    )
                                    page_text_locations.append(text_location)
                                    page_full_text += text + " "

                # í˜ì´ì§€ë³„ ìœ„ì¹˜ ì •ë³´ ì €ì¥
                page_locations[page_num + 1] = page_text_locations

                if page_full_text.strip():
                    page_full_text_normalized = self.normalize_text(page_full_text)  # ğŸ”¥ ì •ê·œí™” ì¶”ê°€
                    text_content.append({
                        "page_number": page_num + 1,
                        "text": page_full_text_normalized,  # ğŸ”¥ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        "char_count": len(page_full_text_normalized),
                        "text_locations": page_text_locations
                    })

            # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
            total_text = " ".join([page["text"] for page in text_content])

            # ê²°ê³¼ ì¤€ë¹„ (ë¬¸ì„œê°€ ì—´ë¦° ìƒíƒœì—ì„œ)
            result = {
                "status": "success",
                "total_pages": total_pages,
                "text_pages": len(text_content),
                "total_characters": len(total_text),
                "full_text": total_text,
                "page_contents": text_content,
                "page_locations": page_locations,
                "extracted_at": datetime.now().isoformat()
            }

            return result

        except Exception as e:
            logger.error(f"ìœ„ì¹˜ ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "extracted_at": datetime.now().isoformat()
            }
        finally:
            # ë¬¸ì„œê°€ ì—´ë¦° ìƒíƒœë¼ë©´ ë°˜ë“œì‹œ ë‹«ê¸°
            if doc is not None:
                try:
                    doc.close()
                except:
                    pass

    def find_text_in_pdf(self, file_path: str, search_text: str, context_chars: int = 100) -> List[TextLocation]:
        """PDFì—ì„œ íŠ¹ì • í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ì°¾ê¸° (ê°œì„ ëœ í•œê¸€ ê²€ìƒ‰)"""
        doc = None
        try:
            doc = fitz.open(file_path)
            locations = []

            # ğŸ”¥ ê²€ìƒ‰ì–´ ì •ê·œí™”
            search_text_normalized = self.normalize_text(search_text)
            logger.info(f"ê²€ìƒ‰ ì‹œì‘: íŒŒì¼ '{file_path}', ì›ë³¸ ê²€ìƒ‰ì–´ '{search_text}', ì •ê·œí™”ëœ ê²€ìƒ‰ì–´ '{search_text_normalized}'")

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_full_text = page.get_text("text")

                # ğŸ”¥ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì •ê·œí™”
                page_text_normalized = self.normalize_text(page_full_text)

                logger.debug(f"í˜ì´ì§€ {page_num + 1} í…ìŠ¤íŠ¸ (ì¼ë¶€): {page_text_normalized[:200]}...")

                # ğŸ”¥ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¼ë¦¬ ë¹„êµ
                if search_text_normalized.lower() in page_text_normalized.lower():
                    logger.info(f"í˜ì´ì§€ {page_num + 1}ì—ì„œ í…ìŠ¤íŠ¸ ë°œê²¬!")

                    # PyMuPDFì˜ search_forë¡œ ì •í™•í•œ ìœ„ì¹˜ ì°¾ê¸° (ì›ë³¸ ê²€ìƒ‰ì–´ ì‚¬ìš©)
                    text_instances = page.search_for(search_text)

                    # ğŸ”¥ ì›ë³¸ìœ¼ë¡œ ëª» ì°¾ìœ¼ë©´ ì •ê·œí™”ëœ ê²€ìƒ‰ì–´ë¡œ ì‹œë„
                    if not text_instances:
                        text_instances = page.search_for(search_text_normalized)
                        logger.info(f"ì •ê·œí™”ëœ ê²€ìƒ‰ì–´ë¡œ ì¬ì‹œë„: {len(text_instances)}ê°œ ë°œê²¬")

                    logger.info(f"í˜ì´ì§€ {page_num + 1}ì—ì„œ '{search_text}' ê²€ìƒ‰ ê²°ê³¼: {len(text_instances)}ê°œ ë°œê²¬")

                    if text_instances:
                        for rect in text_instances:
                            # ğŸ”¥ ë¬¸ë§¥ ì¶”ì¶œ ê°œì„ 
                            match_start_idx = page_text_normalized.lower().find(search_text_normalized.lower())
                            context = ""
                            if match_start_idx != -1:
                                start_idx = max(0, match_start_idx - context_chars)
                                end_idx = min(len(page_full_text),
                                              match_start_idx + len(search_text_normalized) + context_chars)
                                context = page_full_text[start_idx:end_idx]

                            location = TextLocation(
                                page_number=page_num + 1,
                                bbox=(rect.x0, rect.y0, rect.x1, rect.y1),
                                text=search_text_normalized,  # ğŸ”¥ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
                                context=context
                            )
                            locations.append(location)

            return locations

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
        finally:
            if doc is not None:
                try:
                    doc.close()
                except:
                    pass

    def split_text_with_locations(
            self,
            extraction_result: Dict,
            chunk_size: int = 1000,
            chunk_overlap: int = 200
    ) -> List[ChunkLocation]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ë©´ì„œ ìœ„ì¹˜ ì •ë³´ ìœ ì§€"""
        if extraction_result["status"] != "success":
            return []

        full_text = extraction_result["full_text"]
        page_contents = extraction_result["page_contents"]

        chunks_with_locations = []
        start = 0

        while start < len(full_text):
            # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
            end = start + chunk_size

            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°, ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(full_text):
                # ğŸ”¥ í•œê¸€ ë¬¸ì¥ ë¶€í˜¸ í¬í•¨
                sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n']
                for i in range(end, max(start + chunk_size - 100, start), -1):
                    if full_text[i] in sentence_endings or full_text[i].isspace():
                        end = i + 1
                        break

            chunk_text = full_text[start:end].strip()
            if not chunk_text:
                break

            # ì²­í¬ê°€ ì–´ëŠ í˜ì´ì§€ë“¤ì— ê±¸ì³ ìˆëŠ”ì§€ ì°¾ê¸°
            chunk_pages = self._find_chunk_pages(chunk_text, page_contents)

            # ì²­í¬ í•´ì‹œ ìƒì„± (ì¶”í›„ ê²€ìƒ‰ìš©)
            chunk_hash = hashlib.md5(chunk_text.encode('utf-8')).hexdigest()  # ğŸ”¥ utf-8 ì¸ì½”ë”© ëª…ì‹œ

            chunk_location = ChunkLocation(
                chunk_text=chunk_text,
                page_locations=[],  # í•„ìš”ì‹œ ì„¸ë¶€ ìœ„ì¹˜ ì¶”ê°€
                start_page=min(chunk_pages) if chunk_pages else 1,
                end_page=max(chunk_pages) if chunk_pages else 1,
                chunk_hash=chunk_hash
            )

            chunks_with_locations.append(chunk_location)

            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜
            start = max(start + chunk_size - chunk_overlap, end)

            if start >= len(full_text):
                break

        return chunks_with_locations

    def _find_chunk_pages(self, chunk_text: str, page_contents: List[Dict]) -> List[int]:
        """ì²­í¬ í…ìŠ¤íŠ¸ê°€ ì–´ëŠ í˜ì´ì§€ë“¤ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì°¾ê¸° (í•œê¸€ ê°œì„ )"""
        chunk_text_normalized = self.normalize_text(chunk_text.lower())
        chunk_words = set(chunk_text_normalized.split())
        chunk_pages = []

        for page in page_contents:
            page_text_normalized = self.normalize_text(page["text"].lower())
            page_words = set(page_text_normalized.split())

            # ì²­í¬ ë‹¨ì–´ì˜ ìƒë‹¹ ë¶€ë¶„ì´ í˜ì´ì§€ì— ìˆëŠ”ì§€ í™•ì¸
            if chunk_words and page_words:
                overlap = len(chunk_words & page_words)
                overlap_ratio = overlap / len(chunk_words)

                if overlap_ratio > 0.3:  # 30% ì´ìƒ ê²¹ì¹˜ë©´ í•´ë‹¹ í˜ì´ì§€ë¡œ ê°„ì£¼
                    chunk_pages.append(page["page_number"])

        return chunk_pages

    def create_highlight_annotations(
            self,
            file_path: str,
            text_locations: List[TextLocation],
            output_path: str = None
    ) -> str:
        """PDFì— í•˜ì´ë¼ì´íŠ¸ ì£¼ì„ ì¶”ê°€"""
        doc = None
        try:
            doc = fitz.open(file_path)

            for location in text_locations:
                if location.page_number <= len(doc):  # ğŸ”¥ í˜ì´ì§€ ë²”ìœ„ ì²´í¬ ì¶”ê°€
                    page = doc.load_page(location.page_number - 1)

                    # í•˜ì´ë¼ì´íŠ¸ ì£¼ì„ ì¶”ê°€
                    rect = fitz.Rect(location.bbox)
                    highlight = page.add_highlight_annot(rect)
                    highlight.set_colors(stroke=[1, 1, 0])  # ë…¸ë€ìƒ‰
                    highlight.update()

            # íŒŒì¼ ì €ì¥
            if not output_path:
                base, ext = os.path.splitext(file_path)
                output_path = f"{base}_highlighted{ext}"

            doc.save(output_path)
            logger.info(f"í•˜ì´ë¼ì´íŠ¸ëœ PDF ì €ì¥: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"í•˜ì´ë¼ì´íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return file_path
        finally:
            if doc is not None:
                try:
                    doc.close()
                except:
                    pass

    def extract_and_chunk_with_locations(
            self,
            file_path: str,
            chunk_size: int = 1000,
            chunk_overlap: int = 200
    ) -> Dict:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìœ„ì¹˜ ì •ë³´ì™€ í•¨ê»˜ ì²­í¬ë¡œ ë¶„í• """
        try:
            # 1. ìœ„ì¹˜ ì •ë³´ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extract_result = self.extract_text_with_locations(file_path)

            if extract_result["status"] != "success":
                return extract_result

            # 2. ìœ„ì¹˜ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì²­í¬ ë¶„í• 
            chunks_with_locations = self.split_text_with_locations(
                extract_result, chunk_size, chunk_overlap
            )

            # 3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata_result = self.extract_metadata(file_path)

            # 4. ì²­í¬ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            chunks_data = []
            for i, chunk_loc in enumerate(chunks_with_locations):
                chunks_data.append({
                    "chunk_index": i,
                    "text": chunk_loc.chunk_text,
                    "start_page": chunk_loc.start_page,
                    "end_page": chunk_loc.end_page,
                    "chunk_hash": chunk_loc.chunk_hash,
                    "char_count": len(chunk_loc.chunk_text)
                })

            return {
                "status": "success",
                "text_extraction": extract_result,
                "chunks": [chunk.chunk_text for chunk in chunks_with_locations],
                "chunks_with_locations": chunks_data,
                "chunk_count": len(chunks_with_locations),
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "metadata": metadata_result.get("pdf_metadata", {}),
                "processed_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"ìœ„ì¹˜ ì •ë³´ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "processed_at": datetime.now().isoformat()
            }

    def extract_metadata(self, file_path: str) -> Dict:
        """PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        doc = None
        try:
            doc = fitz.open(file_path)
            metadata = doc.metadata

            file_stats = os.stat(file_path)

            # ë¬¸ì„œê°€ ì—´ë¦° ìƒíƒœì—ì„œ ì •ë³´ ìˆ˜ì§‘
            page_count = len(doc)
            is_encrypted = doc.needs_pass
            is_pdf = doc.is_pdf
            version = getattr(doc, 'pdf_version', 'Unknown')

            result = {
                "status": "success",
                "file_info": {
                    "filename": os.path.basename(file_path),
                    "file_size": file_stats.st_size,
                    "created_at": datetime.fromtimestamp(file_stats.st_ctime).isoformat(),
                    "modified_at": datetime.fromtimestamp(file_stats.st_mtime).isoformat()
                },
                "pdf_metadata": {
                    "title": metadata.get("title", ""),
                    "author": metadata.get("author", ""),
                    "subject": metadata.get("subject", ""),
                    "creator": metadata.get("creator", ""),
                    "producer": metadata.get("producer", ""),
                    "creation_date": metadata.get("creationDate", ""),
                    "modification_date": metadata.get("modDate", "")
                },
                "document_info": {
                    "page_count": page_count,
                    "is_encrypted": is_encrypted,
                    "is_pdf": is_pdf,
                    "version": version
                },
                "extracted_at": datetime.now().isoformat()
            }

            return result

        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "extracted_at": datetime.now().isoformat()
            }
        finally:
            if doc is not None:
                try:
                    doc.close()
                except:
                    pass

    def download_and_process_pdf(
            self,
            url: str,
            chunk_size: int = 1000,
            chunk_overlap: int = 200
    ) -> Dict:
        """URLì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìœ„ì¹˜ ì •ë³´ì™€ í•¨ê»˜ ì²˜ë¦¬"""
        temp_file = None
        try:
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tf:
                temp_file = tf.name

            # PDF ë‹¤ìš´ë¡œë“œ
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
            response = requests.get(url, timeout=30)
            response.raise_for_status()

            with open(temp_file, 'wb') as f:
                f.write(response.content)

            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(response.content)} bytes")

            # íŒŒì¼ ì‹œìŠ¤í…œì´ ì™„ì „íˆ ë°˜ì˜ë  ì‹œê°„ì„ ì£¼ê¸° ìœ„í•´ ì§€ì—° ì¶”ê°€
            time.sleep(0.1)

            # PDF ì²˜ë¦¬ (ìœ„ì¹˜ ì •ë³´ í¬í•¨)
            result = self.extract_and_chunk_with_locations(temp_file, chunk_size, chunk_overlap)
            result["download_info"] = {
                "url": url,
                "file_size": len(response.content),
                "downloaded_at": datetime.now().isoformat(),
                "temp_file": temp_file  # í•˜ì´ë¼ì´íŠ¸ ê¸°ëŠ¥ì„ ìœ„í•´ ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìœ ì§€
            }

            return result

        except requests.RequestException as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}",
                "url": url
            }
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "url": url
            }
        # ì„ì‹œ íŒŒì¼ì€ í•˜ì´ë¼ì´íŠ¸ ê¸°ëŠ¥ì„ ìœ„í•´ ìœ ì§€ (ë³„ë„ ì •ë¦¬ í•„ìš”)

    def create_rag_response_with_locations(
            self,
            file_path: str,
            matched_chunks: List[Dict],
            user_question: str
    ) -> Dict:
        """RAG ì‘ë‹µì— í˜ì´ì§€ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€ (ê°œì„ ëœ ë²„ì „)"""
        try:
            # ğŸ”¥ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(file_path):
                logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return {
                    "status": "failed",
                    "error": f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
                }

            response_data = {
                "answer": "",
                "sources": [],
                "page_references": [],
                "highlight_info": []
            }

            for chunk_data in matched_chunks:
                chunk_text = chunk_data.get("text", "")
                chunk_metadata = chunk_data.get("metadata", {})

                # ğŸ”¥ ê°œì„ ëœ ê²€ìƒ‰ ë¡œì§ - ë‹¨ê³„ë³„ ê²€ìƒ‰
                text_locations = []

                # 1ì°¨: ì²­í¬ì˜ ì²« ë¬¸ì¥ìœ¼ë¡œ ê²€ìƒ‰ (ë” ì •í™•í•¨)
                sentences = chunk_text.split('.')
                if sentences:
                    first_sentence = sentences[0].strip()
                    if len(first_sentence) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                        text_locations = self.find_text_in_pdf(file_path, first_sentence)
                        logger.debug(f"1ì°¨ ê²€ìƒ‰ (ì²« ë¬¸ì¥): '{first_sentence}' -> {len(text_locations)}ê°œ ë°œê²¬")

                # 2ì°¨: ì²« ë¬¸ì¥ìœ¼ë¡œ ì•ˆ ë˜ë©´ í‚¤ì›Œë“œ ì¶”ì¶œí•´ì„œ ê²€ìƒ‰
                if not text_locations:
                    words = chunk_text.split()
                    if len(words) >= 3:
                        keyword_phrase = ' '.join(words[:3])  # ì²« 3ë‹¨ì–´
                        text_locations = self.find_text_in_pdf(file_path, keyword_phrase)
                        logger.debug(f"2ì°¨ ê²€ìƒ‰ (í‚¤ì›Œë“œ): '{keyword_phrase}' -> {len(text_locations)}ê°œ ë°œê²¬")

                # 3ì°¨: ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ì²­í¬ì˜ ì•ë¶€ë¶„ìœ¼ë¡œ ê²€ìƒ‰
                if not text_locations:
                    search_snippet = chunk_text[:50].strip()  # ğŸ”¥ 50ìë¡œ ì¶•ì†Œ
                    if search_snippet:
                        text_locations = self.find_text_in_pdf(file_path, search_snippet)
                        logger.debug(f"3ì°¨ ê²€ìƒ‰ (ì•ë¶€ë¶„): '{search_snippet}' -> {len(text_locations)}ê°œ ë°œê²¬")

                if text_locations:
                    primary_location = text_locations[0]

                    source_info = {
                        "chunk_text": chunk_text[:300] + "..." if len(chunk_text) > 300 else chunk_text,
                        "page_number": primary_location.page_number,
                        "bbox": primary_location.bbox,
                        "context": primary_location.context,
                        "relevance_score": chunk_data.get("score", 0.0)
                    }

                    response_data["sources"].append(source_info)
                    response_data["page_references"].append(primary_location.page_number)
                    response_data["highlight_info"].append({
                        "page": primary_location.page_number,
                        "bbox": primary_location.bbox,
                        "text": text_locations[0].text[:100] if text_locations else chunk_text[:50]
                    })
                else:
                    # ğŸ”¥ ìœ„ì¹˜ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°ì—ë„ ì •ë³´ ì¶”ê°€ (í˜ì´ì§€ ì—†ì´)
                    logger.warning(f"ì²­í¬ ìœ„ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {chunk_text[:50]}...")
                    source_info = {
                        "chunk_text": chunk_text[:300] + "..." if len(chunk_text) > 300 else chunk_text,
                        "page_number": chunk_metadata.get("page", 1),  # ë©”íƒ€ë°ì´í„°ì—ì„œ í˜ì´ì§€ ì •ë³´ ì‚¬ìš©
                        "bbox": None,
                        "context": "",
                        "relevance_score": chunk_data.get("score", 0.0)
                    }
                    response_data["sources"].append(source_info)

            # ğŸ”¥ ì¤‘ë³µ í˜ì´ì§€ ì œê±° (None ì œì™¸)
            page_refs = [ref for ref in response_data["page_references"] if ref is not None]
            response_data["page_references"] = list(set(page_refs))

            return {
                "status": "success",
                "rag_response": response_data,
                "processed_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"RAG ì‘ë‹µ ìœ„ì¹˜ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e)
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
pdf_processor = PDFProcessor()


# í¸ì˜ í•¨ìˆ˜ë“¤ (RAG/RAGAS ëŒ€ì‘)
def process_pdf_file_with_locations(file_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict:
    """PDF íŒŒì¼ ì²˜ë¦¬ (ìœ„ì¹˜ ì •ë³´ í¬í•¨)"""
    return pdf_processor.extract_and_chunk_with_locations(file_path, chunk_size, chunk_overlap)


def process_pdf_url_with_locations(url: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict:
    """PDF íŒŒì¼ ì²˜ë¦¬ (URL, ìœ„ì¹˜ ì •ë³´ í¬í•¨)"""
    return pdf_processor.download_and_process_pdf(url, chunk_size, chunk_overlap)


def find_text_locations(file_path: str, search_text: str) -> List[TextLocation]:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸° (ê°œì„ ëœ í•œê¸€ ê²€ìƒ‰)"""
    return pdf_processor.find_text_in_pdf(file_path, search_text)


def create_highlighted_pdf(file_path: str, text_locations: List[TextLocation], output_path: str = None) -> str:
    """í•˜ì´ë¼ì´íŠ¸ëœ PDF ìƒì„±"""
    return pdf_processor.create_highlight_annotations(file_path, text_locations, output_path)


def create_rag_response_with_pages(file_path: str, matched_chunks: List[Dict], question: str) -> Dict:
    """RAG ì‘ë‹µì— í˜ì´ì§€ ì •ë³´ ì¶”ê°€ (ê°œì„ ëœ ë²„ì „)"""
    return pdf_processor.create_rag_response_with_locations(file_path, matched_chunks, question)


# ê¸°ì¡´ í•¨ìˆ˜ë“¤ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
def process_pdf_file(file_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict:
    """PDF íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë²„ì „)"""
    result = pdf_processor.extract_and_chunk_with_locations(file_path, chunk_size, chunk_overlap)
    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    if result["status"] == "success":
        return {
            "status": "success",
            "chunks": result["chunks"],
            "chunk_count": result["chunk_count"],
            "processed_at": result["processed_at"]
        }
    return result


def validate_pdf_file(file_path: str) -> bool:
    """PDF íŒŒì¼ ìœ íš¨ì„± ê²€ì¦"""
    return pdf_processor.validate_pdf(file_path)


def extract_pdf_metadata(file_path: str) -> Dict:
    """PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    return pdf_processor.extract_metadata(file_path)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ ìƒì„± ë° ì²˜ë¦¬
    test_pdf_path = "test_document.pdf"

    try:
        # PDF ìƒì„± - ìˆ˜ì •ëœ ë°©ì‹
        doc = fitz.open()  # with ë¬¸ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        page = doc.new_page()
        text = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ PDF ë¬¸ì„œì…ë‹ˆë‹¤. PDF ì²˜ë¦¬ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. RAGì™€ RAGASë¥¼ ìœ„í•œ ìœ„ì¹˜ ì •ë³´ë„ í¬í•¨ë©ë‹ˆë‹¤."
        page.insert_text((100, 100), text)
        doc.save(test_pdf_path)
        doc.close()  # ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°

        # íŒŒì¼ ì‹œìŠ¤í…œì´ ì™„ì „íˆ ë°˜ì˜ë  ì‹œê°„ì„ ì£¼ê¸° ìœ„í•´ ì§€ì—° ì¶”ê°€
        time.sleep(0.2)  # ì§€ì—° ì‹œê°„ ì¦ê°€

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(test_pdf_path):
            print(f"ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ PDF íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {test_pdf_path}")
            exit(1)

        # ìœ„ì¹˜ ì •ë³´ í¬í•¨ PDF ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        result = process_pdf_file_with_locations(test_pdf_path, chunk_size=100, chunk_overlap=20)
        print("--- ìœ„ì¹˜ ì •ë³´ í¬í•¨ PDF ì²˜ë¦¬ ê²°ê³¼ ---")
        print(f"ìƒíƒœ: {result['status']}")
        if result['status'] == 'success':
            print(f"ì´ í˜ì´ì§€ ìˆ˜: {result['text_extraction']['total_pages']}")
            print(f"ì´ ë¬¸ì ìˆ˜: {result['text_extraction']['total_characters']}")
            print(f"ì²­í¬ ìˆ˜: {result['chunk_count']}")
            if result['chunks_with_locations']:
                print(f"ì²« ë²ˆì§¸ ì²­í¬ í…ìŠ¤íŠ¸: {result['chunks_with_locations'][0]['text'][:50]}...")
                print(
                    f"ì²« ë²ˆì§¸ ì²­í¬ ìœ„ì¹˜: í˜ì´ì§€ {result['chunks_with_locations'][0]['start_page']} - {result['chunks_with_locations'][0]['end_page']}")

            # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸° í…ŒìŠ¤íŠ¸
            search_term = "í…ŒìŠ¤íŠ¸"
            locations = find_text_locations(test_pdf_path, search_term)
            print(f"\n--- '{search_term}' í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸° ê²°ê³¼ ---")
            print(f"'{search_term}' í…ìŠ¤íŠ¸ {len(locations)}ê°œ ë°œê²¬")

            if locations:
                for i, loc in enumerate(locations):
                    print(
                        f"  ìœ„ì¹˜ {i + 1}: í˜ì´ì§€ {loc.page_number}, BBox: {loc.bbox}, í…ìŠ¤íŠ¸: '{loc.text}', ë¬¸ë§¥: '{loc.context[:50]}...'")

                # í•˜ì´ë¼ì´íŠ¸ PDF ìƒì„± í…ŒìŠ¤íŠ¸
                highlighted_path = create_highlighted_pdf(test_pdf_path, locations)
                print(f"\n--- í•˜ì´ë¼ì´íŠ¸ëœ PDF ìƒì„± ê²°ê³¼ ---")
                print(f"í•˜ì´ë¼ì´íŠ¸ëœ PDF: {highlighted_path}")
                print(f"ì´ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í•˜ì´ë¼ì´íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                print(f"'{search_term}' í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            metadata_result = extract_pdf_metadata(test_pdf_path)
            print(f"\n--- PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ê²°ê³¼ ---")
            if metadata_result['status'] == 'success':
                print(json.dumps(metadata_result['pdf_metadata'], indent=2, ensure_ascii=False))
            else:
                print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {metadata_result['error']}")

        else:
            print(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {result['error']}")

    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        print(f"ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {traceback.format_exc()}")
    finally:
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        for test_file in [test_pdf_path, "test_document_highlighted.pdf", "downloaded_highlighted.pdf"]:
            if os.path.exists(test_file):
                try:
                    os.remove(test_file)
                    print(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ: {test_file}")
                except:
                    print(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {test_file}")